% !TEX root = SocialVision2012.tex
\vspace{-5pt}
\subsection{Datasets and Challenge Problems}
\label{sec:sys}
\vspace{-5pt}

%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=\columnwidth]{prototype_1}
%\end{center}
%\vspace{-0.25in} \caption{\captionsize 
%Interactive classrooms are attractive research testbeds because unlike most existing datasets (b: \cite{UTdata}) they contain diverse behaviors in a real, cluttered environments, and therefore provide strong proxies for social visual analysis in the wild (c: \cite{WangMG09}). We will evaluate our research systems on videos collected from a classroom observation system (a) that already exists at Harvard University.}\label{fig:prototype}\end{figure}



%Our goal is to build a foundation for social analysis in diverse, unconstrained environments like Fig.~\ref{fig:prototype}(c). This is becoming well within reach due to increasing availability of networked camera arrays and advances in practical multi-view detection and tracking (e.g.,~\cite{EshelM10}). These unconstrained environments are fundamentally different from scenarios considered in most existing benchmark datasets for interaction analysis (Fig.~\ref{fig:prototype}(b)), where the interactions involve a pre-determined number of participants; take place in the absence of by-standers or social clutter; and/or are localized  \emph{a priori} in time~\cite{UTdata,Choi:context,Choi:recogtrack,CRIM13}. Because of these  differences, evaluations on existing benchmarks fail to provide meaningful proxies for fundamental progress in widespread social visual analysis.


An encouraging trend over the last several years has been the increasing use of benchmark datasets for evaluating progress on particular tasks.\comment{Standardized datasets, such as the Middlebury Stereo Evaluation project~\cite{ScharsteinS02}, the PASCAL VOC Challenges~\cite{Everingham10}, and the Hollywood database of individual actions~\cite{LaptevMSR08},} They have served as catalysts for progress on difficult problems because they create concrete targets that inspire new ideas and allow researchers to evaluate their systems by quantitatively demonstrating improvement. Our goal is to build a foundation and similar benchmarks for social analysis in diverse, unconstrained environments, which is becoming well within reach due to increasing availability of online and socialized imagery as well as networked camera arrays (e.g.,\cite{CamNetRoy}) and advances in practical multi-view detection and tracking (e.g.,~\cite{EshelM10,CamNetSclaroff}). These unconstrained environments are fundamentally different from scenarios in\comment{considered in the few number of} existing datasets for interaction analysis where the interactions involve a pre-determined number of participants; take place in the absence of by-standers or social clutter; and/or are localized \emph{a priori} in space and time~\cite{UTdata,Choi:recogtrack}. Because of these differences, they are not appropriate for problems related to social visual analysis, and evaluations on existing benchmarks fail to provide meaningful proxies for fundamental progress in widespread social visual analysis. 

%As part of the proposed activity, we will create new datasets and challenge problems that make it possible to measure meaningful progress on social visual analysis, and at the same time, serve as mechanisms to engage the broader research community. To ensure our benchmarks provide effective measures of progress, they will be annotated with ground-truth information about identities, interaction categories, and social relationships. We will create and use two distinct types of datasets. The first type will be based on video collections that are already being collected in interactive classrooms at Harvard University. These datasets will be large (more than 350 camera-hours so far) and will enable evaluation of all aspects of the proposed research program. Because of privacy restrictions they will not be made directly available to non-collaborating research groups, so we will create a second set of  datasets for this purpose.


As part of the proposed activity, we will create new datasets that make it possible to measure meaningful progress on social visual analysis, and at the same time, serve as mechanisms to engage the broader research community, by enabling the evaluation of certain aspects of the proposed research program (e.g., detecting and recognizing proxemes). These will be presented as ``challenge problems" for the broader computer vision research community. To ensure our benchmarks provide effective measures of progress, these datasets will be annotated with ground-truth information about identities, social relationships, and proxemes when available. We will also publish side-by-side comparisons of our performance on publicly-shared benchmarks or their alternative/subsets to be explained below. This is a strategy that PI Zickler has previously employed for research on private photo collections~\cite{PintoZickler2011}, and it means that any researcher who evaluates their algorithms on the other public datasets can obtain an estimate of how well those same algorithms might perform in a real social environment.

We will create and use four distinct datasets --- two sets of images and two sets of videos. In the first image set there will be photos depicting co-occurrences of individuals in various social environments that are collected from internet by keyword search as those in Fig~\ref{flicker}, using which we will learn and detect socially informative proxemes\comment{from pairwise poses and other visual cues, but without estimating social networks}. The second image set will be personal photo albums from Facebook as used in~\cite{Stone2008,Stone2010,PintoZickler2011} where social network information is available, on which we will implement the entire framework introduced .in this proposal. In companion, the first video set will be appropriate seasons and/or episodes of selected TV shows or movies (as shown in Fig.~\ref{fig:socialbehavior}(a)) in which a well-shaped social network exists among characters. Finally, the second video set will be the video collections that are already being collected in interactive classrooms at Harvard University and we will use it as a testbed for reconstructing a specialized social network consisting of class of students. These large-scale datasets will enable evaluation of all aspects of the proposed research program. 

Because of copyright restrictions, we will make available the annotations for the TV-show/movie dataset but not the original videos, and interested researchers may directly purchase their copies of videos. Because of privacy restrictions, Facebook albums and Harvard Interactive Classroom Dataset may not be made directly available to non-collaborating research groups, and we will create their alternative sets or subsets that public can share. In the following, we provide more introduction to the Harvard Interactive Classroom Dataset that we have been collecting and preliminarily evaluated in \cite{groupdet2013} by the PIs, as well as the Facebook Photo Dataset \cite{Stone2008} by PI Zickler, which serves as a motivating prototype for the new Facebook dataset to be collected.


\boldstart{Harvard Interactive Classroom Dataset}. We will leverage video that has been being collected by a six-camera array in a large classroom at Harvard University. This system has been developed over the past few years with funding by the NSF (IIS-0835338, 2009--2012) and the Harvard Initiative for Learning and Teaching (HILT)\footnote{\href{http://hilt.harvard.edu/2012-2013-awards}{http://hilt.harvard.edu/2012-2013-awards}} with the goal of understanding how students learn in interactive classrooms. The observed classroom is ``interactive'' in that students frequently engage in ad-hoc discussions. One commonly used technique is \emph{Peer Instruction}, which involves brief student discussions during the time traditionally devoted to lecture. Each discussion begins with a single ConcepTest---a question designed to elicit common student misconceptions~\cite{Crouch:PI,Mazur:PI}. The students are given a moment to formulate and electronically submit their individual answer\comment{to the question posed}, and then they are asked to form ad-hoc discussion groups to try to convince neighboring students of the correctness of their answers. After a few minutes of peer-to-peer discussion, students submit their possibly-revised answers, and this entire activity is usually followed by the instructor’s reinforcement of the main concept. Education research shows that both high and low ability students benefit from these discussions~\cite{Fagen2002,Hake1998,Okebukola1984,Peterson1979}.

%The videos collected by this system present an extraordinary opportunity for developing and evaluating large-scale social visual analysis. The observed classrooms---part of one is shown in Fig.~\ref{fig:prototype}(a)---contain a hundred or more individuals engaged in ad-hoc interactions, and this creates a social environment that is fundamentally different from previous datasets (e.g., Fig.~\ref{fig:prototype}(b)) in which interactions are  pre-localized in space or time. The utility of our interactive classrooms for social visual research stems not only from the number and diversity of people they contain, but from the nature of the interactions, the duration of our observations, and the non-visual sources of metadata available for validation. For each course, videos are collected in every lecture over three-month semesters (about 200 camera-hours per course), enabling thorough evaluation of our methods for aggregating visual interaction data to infer social network information. This evaluation is made meaningful because of immense efforts being made by educational experts (funded by other sources and using techniques like those in \cite{Scherr2009} to define a small number of semantically-meaningful interaction categories and painstakingly annotate occurrences of these interactions in the videos. This means, for example, that we can use the proposed techniques to discover and detect large numbers of interaction categories and then measure the degree to which they agree with those identified by human experts. Furthermore, in addition to video data, we have access to many non-visual sources of social network information (audio recordings from 48 microphones; educational outcomes; answers to ConcepTests; section assignments; gender; age; ethnicity; housing assignments, etc.) that allow cross-validating the social networks we reconstruct from visual data.

The videos collected by this system present an extraordinary opportunity for developing and evaluating large-scale social visual analysis. The observed interactions---some are shown in Fig.~\ref{fig:diagram_dataset}(b)---involve a hundred or more individuals engaged in ad-hoc discussions, and this creates a social environment that is fundamentally different from previous datasets (e.g., \cite{UTdata,Choi:recogtrack}). The utility of our interactive classrooms for social visual research stems not only from the number and diversity of people they contain, but from the nature of the interactions, the duration of our observations, and the non-visual sources of metadata available for validation. For each course, videos are collected in every lecture over three-month semesters (about 200 camera-hours per course), enabling thorough evaluation of our methods\comment{ for aggregating visual interaction data to infer social network information}. This evaluation is made meaningful because of immense efforts being made by educational experts (funded by other sources and using techniques like those in \cite{Scherr2009} to define a small number of semantically-meaningful interaction categories (proxemes) and painstakingly annotate their occurrences in the videos. This means, for example, that we can use the proposed techniques to discover and detect large numbers of proxemes and then measure the degree to which they agree with those identified by human experts. Furthermore, in addition to video data, we have access to many non-visual sources of social network information (audio recordings from 48 microphones; educational outcomes; answers to ConcepTests; section assignments; gender; age; ethnicity; housing assignments, etc.) that allow cross-validating the social networks we reconstruct from visual data.

We have already developed a  suite of computer vision tools for ``pre-processing'' the dataset. This includes implementations of face detection and tracking, identity recognition, and head pose estimation that together provide large numbers of time-varying individual and pairwise descriptors ($\mathbf{f}$ and $\mathbf{g}$ in Section~\ref{sec:activity})\comment{ for input to our methods}. This dataset is affected by many sources of noise (false detections, missed detections, uncertain identities, uncertain pose) and is therefore a good proxy for many real-world environments. Our existing pre-processing suite is only a starting point for our research; we will continue to develop it during the award period to include richer descriptors such as histograms of flow, space-time interest points, and others that capture elements of body pose and gesture. 

In conjunction with our Institutional Review Board, we have established a protocol in which classroom videos are recorded with consent from students and instructors, and then stored and analyzed on VPN-protected servers. More than 95\% of students also provide consent for their videos to appear in academic publications and presentations, allowing us to disseminate our research results through regular academic channels. But since the classroom videos cannot be made directly available to research groups other than ours, we will promote reproducible research in three different ways. First, we will welcome interested research groups to collaborate with us and evaluate their algorithms by logging into our VPN-protected servers and executing their code. Second, when an improved and richer set of low-level visual descriptors with satisfactory quality is developed, we will be able to share these descriptors as alternatives to the original imagery. Third, we will make use of advancing technologies for privacy-preserving social science research---like those currently being developed by colleagues at Harvard University\footnote{\href{http://privacytools.seas.harvard.edu}{http://privacytools.seas.harvard.edu}}---to enable more direct data sharing\comment{ of classroom data for research purposes}. 

\boldstart{Facebook Photo Dataset}\cite{Stone2008}. The Facebook Photo Dataset was collected from 53 volunteers of active Facebook community members, who agreed to contribute photos and metadata through a web API. Using the API, all photos posted by each volunteer, all photos tagged with any of our volunteers’ Facebook friends, all tags associated with any of these photos, and the network of friendships among our volunteers and their friends were archived. A total of 1.28 million tagged photos were retrieved, and all volunteers and tagged friends number 15,752 individuals in all. From this collection, PI Zickler and other collaborators automatically detected and aligned 438,489 face samples that could be associated with the identity labels manually entered by the volunteers. The Facebook dataset was used as the first type of face dataset with social context and enabled the first ``socially-aware" face recognition system \cite{Stone2008}.

In our proposed research, we will retrieve a new photo collection from appropriate online social platforms, and solve the inverse problem: obtaining social network from images. We will inherit the successful data collecting scheme in \cite{Stone2008} by deploying web-based API to volunteers' account, but the new photo collection will be different from the previous dataset in the following aspects. First, we will look into more appropriate online social platforms that emphasize on photo sharing, so that we may obtain more diverse imagery with richer visual cues, e.g.,bodies and scene contexts, than simply faces. Second, we will try to obtain richer annotations about social semantics as well as other metadata than just friendships, with the consent and help from the volunteers. Third, and most important, we will follow a similar strategy as used for the Harvard Interactive Classroom Dataset in producing an publishable version of this dataset, by allowing access to them through our privacy-preserved server, providing anonymized descriptors and metadata, or borrowing other advancing technologies from privacy-preserving social science research.



%In conjunction with our Institutional Review Board, we have established a protocol in which classroom videos are recorded with consent from students and instructors, and then stored and analyzed on VPN-protected servers. More than 95\% of students also provide consent for their videos to appear in academic publications and presentations, allowing us to disseminate our research results through regular academic channels. But since the classroom videos cannot be made directly available to research groups other than ours, we will promote reproducible research in three different ways. First, as described in the next section, we will create smaller parallel  datasets that are publicly-available and allow other research groups to evaluate algorithms for certain aspects of social visual analysis. Second, we will welcome interested research groups to collaborate with us to evaluate their algorithms on our classroom data by running their code on our VPN-protected servers. Third, whenever possible we will make use of advancing  technologies for privacy-preserving social science research---like those currently being developed by colleagues at Harvard University\footnote{\href{http://privacytools.seas.harvard.edu}{http://privacytools.seas.harvard.edu}}---to enable more direct sharing of classroom data for research purposes.

%\boldstart{Challenge problems}. An encouraging trend over the last several years has been the increasing use of benchmark datasets for evaluating progress on particular tasks. Standardized datasets, such as the Middlebury Stereo Evaluation project~\cite{ScharsteinS02}, the PASCAL VOC Challenges~\cite{Everingham10}, and the Hollywood database of individual actions~\cite{LaptevMSR08}, have served as catalysts for progress on difficult problems because they create concrete targets that inspire new ideas and allow researchers to evaluate their systems by quantitatively demonstrating improvement. Currently, there are relatively few datasets available for problems related to social visual analysis, and those that do exist consider severely constrained environments that are not good proxies for the real world.

%As part of the proposed activity, we will create new datasets and define challenge problems that engage the computer vision research community in our research agenda. These will be collections of annotated videos that contain interactions staged by informed actors, and they will simulate unconstrained environments by including long videos of large social gatherings. They will be annotated with identities, relationships, and interaction categories, and will thereby  enable the evaluation of certain aspects of the proposed research program (e.g., detecting interaction categories). These will be presented as ``challenge problems" for the broader computer vision research community. We will also publish side-by-side comparisons of our performance on these publicly-shared benchmarks and analogous, but larger, benchmarks derived from our private classroom data. This is a strategy that PI Zickler has previously employed for research on private photo collections~\cite{PintoZickler2011}, and it means that any researcher who evaluates their algorithms on the public datasets can obtain an estimate of how well those same algorithms might perform in a  real classroom environment.

%With this established classroom observation system and these fundamental modules, we have successfully collected and processed a large-scale classroom behavior database consisting of 100 video clips in total. The students are seated in a regular lecture hall and are observed by a camera array with non-overlapping fields of view. The classroom is ``interactive'' because at various times throughout the lecture students are invited to engage in ad-hoc group discussions about problems provided by the instructor. The scale of our database is orders of magnitude larger than state-of-the-art computer vision datasets (e.g. those used in \cite{UTdata,Choi:context,Choi:recogtrack}), in the number of individuals (10-50 students per camera), the number of cameras (6), and the amount of time (100 minutes per camera, per recording, equaling over 3,000 minutes in total). Through a combination of face detection and tracking, we obtained noisy tracks for all students in each monocular video, upon which we developed descriptive modules that directly extract descriptiors such as head pose and body motion, we have also implemented a high-level module for behavior analysis based on similarity between two social groups, as depicted in Fig. \ref{fig:prototype}(b). In this module, the behavior of each individual is represented by a combination of the head pose and the motion of torso and arms (using the method of histogram of optical flows). The social behavior of a group is then represented by the configuration in space and time of the behaviors of its participants. With this social activity representation, the module detects a salient social activity from a new video and retrieves similar social activities from our established database. This functionality is illustrated in Fig. \ref{fig:prototype}(b), where our system has discovered a three-way conversation by identifying the participants of this conversation and the time span (several to tens of seconds) of this event. Based on the behavior representation for this space-time social interaction, the system searches the remainder of the database and retrieves a list of exemplars containing similar social behavior, ranked in the descending order of the similarities with the query. In this way, any manual annotations associated with the query video can be propagated to the top-ranking exemplars, and we are now taking this approach to propagate our manual annotations across the whole database. 








%At the current and initial stage of the effort to bring socialized semantics to computer vision, it can be usually the case that we do not have sufficient social contexts to help improving our target/face recognition, and we are not always specific about what are the meaningful social interactive activities that are most informative for social description. Conversely, we may neither know clearly about what exactly we should distill from images to fulfill a network learning task, nor have concrete knowledge about how the multiple views or overlapping community structures of a network will eventually turn out to be. However, we have argued at the beginning and demonstrated through the four proposed research problems the fact that the two aspects assist and benefit from each other, and an overall socially-aware visual analytical system is our ultimate goal. To this end, our final proposed research will include an attempt for a framework that eventually integrate social information and image understanding and allow them to learn and self-build themselves in an evolving and unsupervised manner.





