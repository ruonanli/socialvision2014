% !TEX root = SocialVision2014.tex
\vspace{-5pt}
\subsubsection{Spatio-Temporal Detection and Classification of proxemes}
\label{subsec:activity}
\vspace{-5pt}

The task of learning a proxeme dictionary will be discussed in the next section. For now, let us assume that the dictionary of proxemes, each of which represented by one of more of its exemplars (i.e., representative descriptor-ensembles from one or more interaction categories), are available, and consider the problem of detecting and recognizing new instances of these proxemes in long videos of large social gatherings. We begin by describing a preliminary system reported recently by the PIs~\cite{groupdet2013}. In this system, proxemes are represented by one or more exemplars, and detecting and recognizing new instances of these proxemes is formulated a matching problem, illustrated in Figure~\ref{fig:proxeme_det_recog} and depicted in more detail in the Figure 1 of~\cite{groupdet2013} \footnote{\url{http://gvi.seas.harvard.edu/sites/all/files/Li_Porfilio_Zickler_CVPR2013.pdf}}. We have an exemplar ${\cal E}=\{\mathbf{f},\mathbf{g}\}$ involving $N$ participants over some interval of time, and we wish to localize within a long video of a larger social gathering ($M>N$ actors) the times and locations in which $N$-agent sub-groups participate in similar interactions. To succeed, we must deal with some of the $M$ tracks being artifacts caused by false detections, and others being fragmented due to tracking breakdowns and agent exits. We must also account for interactions occurring over different temporal extents and at variable rates within their extents.

\begin{figure}[t!]
\begin{center}
\includegraphics[width=0.75\columnwidth]{proxeme_det_recog}
\end{center}
\vspace{-0.25in} \caption{\captionsize 
Semantic image processing for detecting and classifying proxemes in image/video collections. (Details about the left box in Figure~\ref{fig:intro}, best viewed in color)\label{fig:proxeme_det_recog}\afterfigspace}
\end{figure}



To deal with these various sources of noise and outliers, we start with each frame of the exemplar being separately compared to each frame of the long input video, and for each comparison we find the optimal partial matching between the $N$ exemplar participants and the $M>N$ input tracks. Each frame-wise partial match is represented by an $N\times M$ binary matrix $W$ that has at most one non-zero entry in each row and column. \comment{Because our descriptor-ensembles include only unitary and pairwise descriptors}Then, each partial match can be found efficiently using a quadratic program\comment{ similar to that used in shape matching \cite{Berg05shapematching}} $\text{arg}\min_{W=\{w_{nm}\}} \sum_{nm}w_{nm}d_{I}(\mathbf{f}_{m}, \mathbf{f}_{n})+\!\!\!\sum_{nmn'm'}\!\!\!w_{nm}w_{n'm'}d_{P}(\mathbf{g}_{m,m'}, \mathbf{g}_{n,n'})$, where $\{\mathbf{f}_{n},\mathbf{g}_{n,n'}\}$ is the collection of descriptors from one exemplar frame and $\{\mathbf{f}_{m},\mathbf{g}_{m,m'}\}$ is that from one input frame. We can explore many different ``atomic'' distances $d_I,d_P$, and so far we have experimented with one flexible and convenient choice based on Mahalanobis distance: 
\begin{equation}\label{eq:partial-matching}
d_{I}(\mathbf{f}, \mathbf{f}')=(\mathbf{f}-\mathbf{f}')^{T}\Sigma_{I}(\mathbf{f}-\mathbf{f}'), d_{P}(\mathbf{g}, \mathbf{g}')=(\mathbf{g}-\mathbf{g}')^{T}\Sigma_{P}(\mathbf{g}-\mathbf{g}')
\end{equation}
with positive semi-definite matrices $\Sigma_{I}$ and $\Sigma_{P}$ that are data-optmized. Having computed all per-frame partial matches between the frames in exemplar ${\cal E}$ and the input video, we employ a sliding window approach that sequentially considers all temporal windows of the input video (with each temporal window $\mathbf{w}$ represented by its descriptor ensemble ${\cal E}_\mathbf{w}$) and: 1) makes a robust decision about the locations of the $N$ best-matching participants in each temporal window by accumulating weighted votes (in the discrete space of all possible $W$ matrices) from the relevant frame-wise matches; 2) optimally refines the start and end points of the window $\mathbf{w}$ using an efficient branch-and-bound search; and 3) ultimately returns a similarity score $S({\cal E},{\cal E}_\mathbf{w})$. We have evaluated the proxeme detection and classification approach using an early precursor to the Harvard Interactive Classroom Dataset that will be described in Section~\ref{sec:sys}. Our results are detailed in~\cite{groupdet2013}, and they show that this preliminary system not only provides useful performance on this challenging classroom dataset, but also achieves state-of-the-art performance on simpler, standard benchmark datasets involving both humans~\cite{UTdata} and animals~\cite{CRIM13}, demonstrating robustness to false detections and insensitive to moderately broken tracks. 

This functionality of associating an interaction to a proxeme instance naturally enables the estimation of the social relationship between the two images targets: As long as the proxeme exemplar ${\cal E}$ represents a proxeme that are strongly modulated by and informative about social relationships --- either those defined by social scientists \cite{hall1974,Kendon1990,kendon1975some,Scherr2009} or those discovered in a data-driven manner as described in~\ref{sec:actlearn} --- it will cast a distribution over relationship types for the interacting targets involved in ${\cal E}_\mathbf{w}$. Denoting the involved targets as $m$ and $n$, the probability $z_{mn}(k)$ of the $k$-th type of relationship between them will be obtained and serve as the input to relationship filtering and network reconstruction as described in \ref{sec:vis2net}. More importantly, the approach also provides a foundation for unsupervised learning of proxemes from data, because it provide a way to compute the similarity $S({\cal E},{\cal E}')$ between any two interaction specimens represented by ensembles ${\cal E}$ and ${\cal E}'$, as discussed in~\ref{sec:actlearn}.





\comment{Through a combination of face detection and tracking, we obtained noisy tracks for all agents and computed simple descriptors based on absolute and pair-wise relative head poses. With input from educational experts we manually defined seven semantically-meaningful proxemes based on head poses, and we manually annotated the participants and temporal extents of 366 distinct instances of these proxemeswithin a much larger video corpus that includes dozens of students observed over many camera-hours. The exemplar interactions range from a few seconds to tens of seconds in length, and the number of by-standers in a video is much larger than the number of participants ($M$ is between 10 and 20 while $N=2,3$).}



\boldstart{Future directions}. While these early results are promising, they are but a small indication of what is achievable: We will explore a number of directions in improving representations of interactions. This includes matching algorithms for higher-order descriptor ensembles, with relative, between-agent descriptors defined not just pairwise but between three- or four-agent cliques, and between each agent and the entire group. We will explore matching algorithms that incorporate loose and flexible models for the spatial and temporal structure within a descriptor-ensemble, as space-time analogies to ~\cite{lazebnik2006beyond,pictorial1973,DPM2013} \comment{ the loose spatial models that have been successful for recognizing scene categories based on distributions of quantized local interest points, and to flexible parts-based models for recognizing object categories}. We will also explore measures of interaction similarity that, instead of being designed for category recognition, are specifically designed as a means to compute ``interaction intensity'' in space and time, as analogy to\comment{measures of ``objectness'' proposed for photographs}~\cite{alexe2012measuring}.

We will also enhance the domain-specific proxeme dictionaries and their associated social semantics. We will develop mechanisms for data-driven discovery of salient proxemes in a new image set or domain where expert knowledge is scarce or unavailable, as enabled by the computable similarity $S({\cal E},{\cal E}')$ and approaches to be introduced next. Very critically, we will enrich the preliminary set of social relationship types considered in earlier social science studies, and bring in a comprehensive taxonomy of relationship categories to associate with the proxeme categories. This taxonomy will be borrowed, for example, from ``WordNet" \cite{MillerWordNet}, a large lexical database of words that are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept and that has been employed by various semantic annotation tasks \cite{TouschSHI}. The taxonomy will provide the set of relationship types, indexed by $k$ in the probability $z_{mn}(k)$, which will be transferred to the social graph edges through the associated salient proxeme as to be discussed in~\ref{sec:vis2net}.








%\begin{wrapfigure}{r}{0.5\textwidth}
%\vspace{-20pt}
%  \begin{center}
%    \includegraphics[width=0.5\textwidth]{groupdet2013}
%  \end{center}
%  \vspace{-20pt}
%  % \caption{Preliminary system for detecting and recognizing social proxemes using the proposed representation.}
%  % \vspace{-15pt}
%%\label{fig:diagram}
%\end{wrapfigure}




%We approach detection and recognition as a matching problem, depicted in the left of Fig.~\ref{fig:diagram_dataset}. Suppose we have an exemplar from an interaction category involving $N$ participants, and we wish to localize within a long video of a larger social gathering ($M>N$ tracks) the times and locations in which sub-groups of agents participate in similar interactions. To succeed, we must deal with some of the $M$ tracks being artifacts caused by false detections, and others being fragmented due to tracking breakdowns and agent exits. We must also account for interactions occurring over different temporal extents and at variable rates within their extents.



%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=\columnwidth]{groupdet2013}
%\end{center}
%\vspace{-0.25in} \caption{\captionsize 
%(a): Preliminary system for detecting and recognizing social proxemes using the proposed representation. (b): Examples of the educationally-meaningful proxemes.}
%\label{fig:diagram_dataset}\afterfigspace
%\end{figure}



%where $\{\mathbf{f}_{n},\mathbf{g}_{n,n'}\}$ is the collection of $N$ individual and $N\times (N-1)$ pairwise descriptors from one exemplar frame and $\{\mathbf{f}_{m},\mathbf{g}_{m,m'}\}$ are the $M$ and $M\times (M-1)$ individual and pairwise descriptors from one frame of the input. 

%\todd{Working here}. To formally describe task consider the fact that if one of the $M$ targets is regarded as a participant in an interaction as exemplified by the exemplar $\mathcal{D}$, its behavior should properly match to the behavior of one of the $N$ individuals in the exemplar $\mathcal{D}$. Therefore, we use a $N\times M$ binary matrix $W=[w_{nm}]\in\{0,1\}^{N\times M}$ to formally represent the participant identification, where $w_{nm}=1$ means that the $n$th exemplar individual is matched to the $m$th input target and $w_{nm}=0$ means unmatched targets. For unambiguous matching, we expect each individual in the exemplar to find its unique partner target in the input. This implies that $\sum_{m}w_{nm}=1, \forall n$ and $\sum_{n}w_{nm}\leq 1, \forall m$, \textit{i.e.}, $W\mathbf{1}=\mathbf{1}$ and $\mathbf{1}^{T}W\leq\mathbf{1}^{T}$. Denote $\mathcal{W}\triangleq\{W\in\{0,1\}^{N\times M}| W\mathbf{1}=\mathbf{1}, \mathbf{1}^{T}W\leq\mathbf{1}^{T}\}$, and our former task is essentially to find $W^{*}\in\mathcal{W}$, which encodes the best matching between the input $\mathcal{Q}$ and the exemplar $\mathcal{D}$. To formally describe task 2) is straightforward: We simply look for the starting time $T_{s}$ and ending time $T_{e}$, $1\le T_{s}<T_{e}\le T$, such that the interactive pattern of input $\mathcal{Q}$ during $[T_{s}, T_{e}]$ demonstrates the best similarity with the exemplar $\mathcal{D}$.


%Suppose that there are Mahalonobis-parameterized atomic distances $d_{I}(\mathbf{f}, \mathbf{f}')=(\mathbf{f}-\mathbf{f}')^{T}\Sigma_{I}(\mathbf{f}-\mathbf{f}')$ and $d_{P}(\mathbf{g}, \mathbf{g}')=(\mathbf{g}-\mathbf{g}')^{T}\Sigma_{P}(\mathbf{g}-\mathbf{g}')$ ($\Sigma_{I}\succeq 0$ and $\Sigma_{P}\succeq 0$ are positive semi-definite matrices) to compare descriptors, and accordingly define the `instantaneous' matching between input $\mathcal{Q}_t$ and exemplar $\mathcal{D}_s$ under matrix $W$ can be defined as 
%\begin{equation}
%\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W)=\sum_{nm}w_{nm}d_{I}(\mathbf{f}_{m,t}, \mathbf{f}^{D}_{n,s})+\!\!\!\!\!\!%\sum_{nmn'm'}\!\!\!\!\!w_{nm}w_{n'm'}d_{P}(\mathbf{g}_{m,m',s}, \mathbf{g}^{D}_{n,n',t}).
%\end{equation}
%Consequently, the optimal instantaneous matching between input $\mathcal{Q}_t$ and exemplar $\mathcal{D}_s$ straightforwardly becomes $W^{t,s}\triangleq\arg\min_{W\in\mathcal{W}}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W)$, and the ensemble distance between them becomes $D(\mathcal{Q}_{t}, \mathcal{D}_{s})\triangleq\min_{W\in\mathcal{W}}\hat{D}(\mathcal{Q}_{t}, \mathcal{D}_{s}, W)$. As a result, our approach, as illustrated in Fig. \ref{fig:diagram_dataset}(a), firstly evaluates all $T\times S$ ensemble distances $D(\mathcal{Q}_{t}, \mathcal{D}_{s})$ together with their 'instantaneous optimal matching' $W^{t,s}$ $\forall 1\le t\le T, 1\le s\le S$. This is followed by a Hough voting procedure to find the best matching $W^{*}$ from all instantaneous optimal matchings $\{W^{t,s}\}$. Finally, we construct from all ensemble distances $\{D(\mathcal{Q}_{t}, \mathcal{D}_{s})\}$a quality function $q(t)$, which achieves a negative value if and only if $t$ falls into the interval where the interaction of interest occurs, and therefore enable an efficient branch-and-bound search estimates the temporal extent $[T_{s}, T_{e}]$.  

%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=\columnwidth]{ROC}
%\end{center}
%\vspace{-0.25in} \caption{\captionsize 
%ROC curves for interaction detection using our proposed approach.\label{fig:dataset_compare}\afterfigspace}
%\end{figure}


%\begin{wrapfigure}[12]{r}{2.4in}
%\vspace{-3.9mm}
%\includegraphics[width=2.4in]{ROC_s}
%\end{wrapfigure}

%%We have evaluated this approach using a subset of the Harvard Interactive Classroom Dataset, to be described in Section~\ref{sec:sys}, which is orders of magnitude larger than existing interaction benchmarks~\cite{UTdata,Choi:context,Choi:recogtrack}), in terms of the number of individuals (10-50 students per camera), the number of cameras (6), and the amount of time (more than 50 camera-hours). Through a combination of face detection and tracking, we obtained noisy tracks for all students in each camera and computed simple descriptors based on absolute and pair-wise relative head pose. With input from educational experts we manually identified the participants and start/end times of 112 distinct three-person discussions, and we separated these into four semantically-meaningful categories (e.g., three sitting in a row with two facing left vs. two in front and one behind). The annotated interactions range from a few seconds to tens of seconds in length, and detecting and recognizing them is a challenge because the number of by-standers is much larger than the number of participants ($M$ is between 10 and 20 while $N=3$), video quality is limited (low light, less than 15fps), and the visual cues for interaction are quite subtle. Detection results obtained using a leave-one-session-out evaluation scheme are shown in the ROC curves at right, with various parts of the system being isolated, leaving only individual $\mathbf{f}$ or pairwise $\mathbf{g}$ descriptors, and atomic distance matrices $\Sigma_{I}$, $\Sigma_{P}$ set to identity instead of being data-optimized as will be described in Section~\ref{sec:actlearn}. These results suggest that our preliminary system approach works well, and its performance and flexibility are further supported by tests showing that it provides state-of-the-art detection and recognition results on existing interaction datasets involving both humans~\cite{UTdata} and mice~\cite{CRIM13}, exceeding the accuracy in~\cite{UTdata} by 21\% and exceeding the accuracy in \cite{CRIM13} by 8\%.



%We have evaluated this approach using a subset of the Harvard Interactive Classroom Dataset, to be described in Section~\ref{sec:sys}, which is orders of magnitude larger than existing interaction benchmarks~\cite{UTdata,Choi:context,Choi:recogtrack}), in terms of the number of individuals (10-50 students per camera), the number of cameras (6), and the amount of time (more than 50 camera-hours). Through a combination of face detection and tracking, we obtained noisy tracks for all students in each camera and computed simple descriptors based on absolute and pair-wise relative head pose. With input from educational experts we annotated the participants and start/end times of 254 two-person and 112 three-person salient interactions separated into seven semantically-meaningful proxemes. The annotated interactions range from a few seconds to tens of seconds in length, and detecting and recognizing them is a challenge because the number of by-standers is much larger than the number of participants ($M$ is between 10 and 20 while $N=2,3$), video quality is limited (low light, less than 15fps), and the visual cues for interaction are subtle. Detailed results \cite{groupdet2013} suggest that our preliminary system works well on this data and also achieves state-of-the-art performances on benchmark datasets involving both humans~\cite{UTdata} and animals~\cite{CRIM13}.  


%While these early results are promising, they are only a small indication of what might be achieved. As part of the proposed activity, we will explore more sophisticated descriptor ensembles that allow, for example, larger cliques of relative descriptors and descriptors that are relative to the entire group. We will also explore measures of similarity that, instead of detection and recognition by matching, are well-suited for computing interaction saliency (i.e., interactions that are distinct from their space-time neighborhood) and for semi-supervised and unsupervised learning of categories (e.g., to automatically cluster the salient interactions discovered in a video collection).



%We will also explore spatio-temporal search algorithms that preserve the robustness to false detections and broken tracks brought by voting but are more efficient in time.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\subsection{Salient Social Activities Discovery and Prediction}
%\label{sec:activity}
%
%The joint target recognition problem described in the previous section represent one scenario where social contexts significantly assist and improve a traditional computer vision task. In this section, we propose a \emph{new} computer vision task motivated and enabled by socialized metadata. This new task introduces new `words' that a picture or a video may convey, and the output conversely produces new clues for social sensing in related disciplines. 
%
%As the second major problem regarding socially-aware visual understanding, we propose to discover and predict salient social activities from videos, as well as still images if possible. The proposed task is novel from existing activity analysis and recognition work in three aspects: It seeks to discover salient social behaviors,  it  aims to predict activities using social contexts, and it directly builds upon realistic visual materials `in the wild'.
%
%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=\columnwidth]{socialbehavior}
%\end{center}
%\vspace{-0.25in} \caption{\captionsize 
%(a): Socially non-informative co-occurrence of articulations\cite{UTdata}; (b): Socially non-informative collective crowd activities\cite{Choi:context,Choi:recogtrack}; (c): Socially informative salient interactions; (d) Socially informative salient group interactions in the nature; (e) Socially meaningful three-way activity categories defined as `f-formations' by sociology\cite{Kendon1990}.\label{fig:socialbehavior}\afterfigspace}
%\end{figure}
%
%
%First, we propose to discover salient social activities. Specifically, social semantics assist in visual understanding in the way that they provide new social behavior categories for us to distill from images and videos, and these new social categories are informed by qualitative and quantitative sociology, which has been studying these behavior modes but never be automated by computer vision. As these new concepts are socially meaningful, they provide more expressive evidence about the social relationships among the individuals involved in the activities. We refer to these new social semantics as salient. In contrast to existing vision research, salient social activities are distinctive because they do not focus on non-informative interactions or group-wise behaviors. The non-informative interactions, for example, refer to the co-occurrences of two individual body articulations with limited clue about how the two individuals are related or exchange opinions, such as 'kick', 'punch', etc. (\cite{UTdata}, Fig. \ref{fig:socialbehavior}(a)). The non-informative group-wise behaviors, on the other hand, include collective activities of a crowd, within which there are no explicit interactions, such as 'group-walking', 'queueing', etc. (\cite{Choi:context,Choi:recogtrack}, \ref{fig:socialbehavior}(b)). Instead, the salient social activities are more  of gestural and conversational interactions, attention-response, as well as turn-taking meetings  (Fig \ref{fig:socialbehavior}(c)) where sociological semantics, as is interesting and important to the sociology research, can be derived. Fig. \ref{fig:socialbehavior} (e) shows diagrams for such socially meaningful activities categories, namely `f-formations', in three-way interactions that have been under the investigation of sociology. Moreover, we will develop generic salient socialized discovering models and approaches, so that they accommodate activities of social populations of animals and insects (Fig. \ref{fig:socialbehavior}(d)).
%
%Second, we propose to predict activities under social contexts. In this case, social semantics assist in visual understanding in the way that they serve as contextual information for restricting the more possible categories of activities that may occur between a specific group of individuals. This effort is also complementary to the usage of social contexts for identifying the targets presented in the previous section. Consider a simple illustrative example in which we would like to label a conversational scene in a movie as either a `negotiationÔ or a 'debate'. It is possible, in this case, that by the analysis of facial expressions, gestures, and poses, we still have difficulty in distinguishing the two. However, by appropriate face recognition in association with other metadata of the movie, probably with the help of social contexts as introduced in the previous section, we may gain solid confidence regarding the social relationship between the speakers as either `cooperative' or 'adverse'. A cooperative relationship are more likely to imply a negotiating activity, and an adverse relationship implies otherwise. A mechanism, similar to and in companion to the CRF formulation but adapted to video analysis, will be particularly useful.
%
%Third, we expect to develop approaches that directly take in realistic visual materials recording social activities of interest. On the one hand, realistic visual materials capture the social activities in unconstrained environment in the format of long-term surveillance videos of public areas with irrelevant human beings co-existing with socially engaged actors as well as scene/background clutter. Our approach will be one that can effectively and efficiently localize and retrieve salient social activities in space and time from these large volumes of cluttered image sequences. On the other hand, realistic visual materials such as surveillance videos and web videos are frequently in low and varying frame-rate, in low resolution, blurry, as well as in adverse view-points. Our effort will be handling all these realistic conditions and providing robust and practical tools that process more than those simulated, high-quality, manually pre-processed data \cite{UTdata,Choi:context,Choi:recogtrack}.
%
%\subsubsection{A prototype system and preliminary results}
%
%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=\columnwidth]{prototype}
%\end{center}
%\vspace{-0.25in} \caption{\captionsize 
%The prototype hardware and software infrastructure for analyzing socialized human behaviors in an indoor environment at Harvard University.\label{fig:prototype}\afterfigspace}
%\end{figure}
%
%As an initial attempt to analyze socialized human behaviors in an indoor environment, we have built up a prototype hardware and software infrastructure at Harvard University. As shown in Fig. \ref{fig:prototype}(a), the hardware part of the prototype system mainly consists of a networked audio-visual recording system for large-scale recording of student interactions in a Harvard College lecture hall. The system records audio and video from approximately one hundred students during each lecture in conjunction with the audience responses through an online teaching-learning application. The video system uses six IP cameras that together provide resolution that is high enough to achieve face-based identity recognition of each student in the audience. The audio system is an innovative design consisting 48 omnidirectional boundary microphones mounted inconspicuously among the seats, and outputting sound from each microphone recording to a single user-friendly mp3 audio file.To automatically analyze the videos, we have developed a robust computational suite of computer vision tools. This software consists of several fundamental modules that directly distill behavioral descriptions from raw videos, as well as a high-level module that detects salient social activities of interest from a new video and retrieves similar social activities. 
%
%
%With this established classroom observation system and these fundamental modules, we have successfully collected and processed a large-scale classroom behavior database consisting of 100 video clips in total. The students are seated in a regular lecture hall and are observed by a camera array with non-overlapping fields of view. The classroom is ``interactive'' because at various times throughout the lecture students are invited to engage in ad-hoc group discussions about problems provided by the instructor. The scale of our database is orders of magnitude larger than state-of-the-art computer vision datasets (e.g. those used in \cite{UTdata,Choi:context,Choi:recogtrack}), in the number of individuals (10-50 students per camera), the number of cameras (6), and the amount of time (100 minutes per camera, per recording, equaling over 3,000 minutes in total). Through a combination of face detection and tracking, we obtained noisy tracks for all students in each monocular video, upon which we developed descriptive modules that directly extract descriptiors such as head pose and body motion, we have also implemented a high-level module for behavior analysis based on similarity between two social groups, as depicted in Fig. \ref{fig:prototype}(b). In this module, the behavior of each individual is represented by a combination of the head pose and the motion of torso and arms (using the method of histogram of optical flows). The social behavior of a group is then represented by the configuration in space and time of the behaviors of its participants. With this social activity representation, the module detects a salient social activity from a new video and retrieves similar social activities from our established database. This functionality is illustrated in Fig. \ref{fig:prototype}(b), where our system has discovered a three-way conversation by identifying the participants of this conversation and the time span (several to tens of seconds) of this event. Based on the behavior representation for this space-time social interaction, the system searches the remainder of the database and retrieves a list of exemplars containing similar social behavior, ranked in the descending order of the similarities with the query. In this way, any manual annotations associated with the query video can be propagated to the top-ranking exemplars, and we are now taking this approach to propagate our manual annotations across the whole database. 
%
%
%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=\columnwidth]{dataset_compare}
%\end{center}
%\vspace{-0.25in} \caption{\captionsize 
%(a-1)-(a-3): Examples of the three categories for two-person interactions; (b-1)-(b-4): Examples of the four categories for three-person interactions; (c) Preliminary performance comparisons of our social activity retrieval software.\label{fig:dataset_compare}\afterfigspace}
%\end{figure}
%
%
%To evaluate the effectiveness of the social behavior retrieval module, we manually identified the participants and start/end times of all two-person, three-person, and four-person interactions, obtaining 254 two-person and 112 three-person interactions in total to constitute the entire database. In consultation with education experts we define interaction categories based on the geometric configurations of the participants: three categories for 2-person interactions (same row; different rows with left agent in front; different rows with right agent in front) and four categories for 3-person interactions. Example images for each category can be found in Fig. \ref{fig:dataset_compare}(a)(b). The annotated interactions range from a few seconds to tens of seconds in length. Since the raw videos arise from five different hour-long session, and we adopt a leave-one-session-out evaluation scheme in partitioning training samples (exemplars) from test samples (inputs). We study classification performance for 2-person and 3-person interactions, where we measure the accuracy of inferred interaction categories. Our low-level modules include both pose descriptors and relative motion descriptors, and our high-level module employs Metric Learning (ML). To evaluate the effectiveness of all these functionalities, we turned off some of components as baselines and compared with the full module with all components on. Fig. \ref{fig:dataset_compare}(c) shows the average true positive rates vs. false positives when classifying detected interactions into the three or four categories. As expected we see that performance improves when more parts of the system turned on, which implies that our prototype infrastructure has achieved reasonable technical quality.
%
%In our proposed research, we aim to enrich the descriptions of the social behaviors in the current prototype system, explore more accurate and robust comparison and retrieval mechanisms, and in particular, design novel modules for integrating social contextual annotations to allow our framework to be fully `socially-aware'.