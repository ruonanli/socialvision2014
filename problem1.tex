% !TEX root = SocialVision2012.tex

\Section{Proposed Research}
\label{sec:proposed-research}

%To the end of our comprehensive framework to sense a social network from visual information, our research agenda on social visual analytics spans two specific major aspects: 1) Detection and recognition of social interaction categories; 2) Social network estimation from multiple visual sources. To facilitate our research activity, we have built up a hardware and software infrastructure as a prototype system and achieved promising preliminary results.

%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=\columnwidth]{image_feature}
%\end{center}
%\vspace{-0.25in} \caption{\captionsize 
%Socially-informative visual cues from still images: (a-1) Original picture with detected faces; (a-2) relative positions of the detections; (a-3) Facial expressions; (a-4) Relative gazes; (a-5) Relative poses; (a-6) scene recognition. \label{fig:prob1}\afterfigspace}
%\end{figure}

%Images and videos of people reveal information about their interactions, roles, and relationships through a variety cues. The still image in the top-left of Fig.~\ref{fig:intro} suggests the two individuals are friends instead of colleagues because of their proximity, their poses and expressions, and the scene category (restaurant/bar) in which they appear. All of these cues are accessible with varying degrees of certainty using existing tools for face detection, pose and expression estimation, and scene recognition; and their utility for social analysis has already been demonstrated by recent efforts to recover information about age and gender~ \cite{Gallagher}, relationship categories (e.g., father-daughter vs. siblings)~\cite{Wang2010} and event categories (e.g., beach party vs.~country bar)~\cite{Murillo2012}.


Imagery of people reveal information about their roles and relationships through depicting socially informative proxemes obtained from a variety of visual cues. The still image in the top-left of Fig.~\ref{fig:intro} suggests the two individuals are friends instead of colleagues because of their proximity, their poses and expressions, and the scene (restaurant/bar) in which they appear. All of these cues are accessible with varying degrees of certainty using existing tools for face detection, pose and expression estimation, and scene recognition; and their utility\comment{ for social analysis} has already been demonstrated by recent efforts to recover information about age and gender~ \cite{Gallagher}, relationship categories (e.g., father-daughter vs. siblings)~\cite{Wang2010} and event categories (e.g., beach party vs.~country bar)~\cite{Murillo2012}.

Videos contain even more information about social proxemes than images, because they provide access to more precise descriptions of social encounters. Sometimes this might be simply a matter of choosing from the video one keyframe that allows clearer categorization of the interaction (shaking hands or hugging in Fig.~\ref{fig:socialbehavior}(a) or three-way conversation in (c)). Other times much more can be revealed by the spatial and temporal structure of approximately-tracked actors, as suggested by Figs.~\ref{fig:socialbehavior}(b--d), where one might detect and recognize attention-response or turn-taking in a conversation; a chance encounter between two friends; or an altercation between two passers-by.

\begin{figure}[b]
\begin{center}
\includegraphics[width=\columnwidth]{socialbehavior_2014}
\end{center}
\vspace{-0.25in} \caption{\captionsize 
Social encounters are ubiquitous (a--d). We will study them and their underlying social relationships by learning and detecting discrete sets of salient environment-specific ``proxeme" categories (e.g., (e)). 
%(a): Socially non-informative co-occurrence of articulations\cite{UTdata}; (b): Socially non-informative collective crowd activities\cite{Choi:context,Choi:recogtrack,WangMG09}; (c)(d): Socially informative salient interactions; (e) Socially meaningful three-way activity categories defined as `f-formations' by sociology\cite{Kendon1990}.
\label{fig:socialbehavior}\afterfigspace}
\end{figure}


%Our goal is to build a computational foundation for social visual analysis by creating tools for extracting as much information as possible about interactions and relationships between people. Our methods will apply to both images, for which social analysis is somewhat established, and videos, for which social analysis is in its infancy; and they will incorporate social interaction categories as a useful intermediate representation between input imagery and the underlying social network. For example, if two individuals are detected as frequently participating in a collegial one-on-one conversation around the water cooler at their workplace, their relationship is likely to be friendly in addition to professional. Similarly, if we observe attention-response instead of turn-taking in a meeting or conversation (Figs.~\ref{fig:socialbehavior}(c--d)) we can recover more detailed information about the interactions and relationships between friends and colleagues.


Our goal is to build a computational foundation for social visual analysis by creating tools for extracting as much information as possible about relationships and social networks among people. Our methods will apply to both images, for which social analysis is somewhat established, and videos, for which social analysis is in its infancy; and they will incorporate detecting and recognizing social proxemes as a useful intermediate representation between input imagery and the underlying social network. For example, if two individuals are detected as frequently participating in a collegial one-on-one conversation around the water cooler at their workplace, their relationship is likely to be friendly in addition to professional. Similarly, if we observe attention-response instead of turn-taking in a meeting or conversation (Figs.~\ref{fig:socialbehavior}(c--d)) we can recover more detailed information about the relationships between friends and colleagues.


%In order to recover social information from image data, we propose a broad set of tools that enable: 1) representing the space of social interactions in an environment by a discrete set of environment-specific categories; 2) automatically recording over time and space the rates at which different individuals participate in these interaction categories; and 3) using these recordings along with the image data to draw inferences about the underlying social network. Our proposal is inspired by research in sociology~\cite{Kendon1990,Hoyle,Tannen}, economics~\cite{econo_category}, and education~\cite{Scherr2009}, where various social environments  have been  analyzed in terms of a small numbers of interaction categories that are salient and semantically-meaningful (e.g., conversational ``f-formations''~\cite{Kendon1990} in Fig.~\ref{fig:socialbehavior}(e)). It is also motivated by our preliminary results, which suggest that semantically-meaningful interaction categories can reliably be detected and recognized in a real-world (classroom) setting.

In order to recover social network from image data, we propose a broad set of tools that enable: 1) representing the space of social interactions in an environment by a discrete set of environment-specific proxemes (categories); 2) automatically recording over time and space the rates at which different individuals participate in these categories; and 3) using these recordings along with the image data to draw inferences about the underlying social network. Our proposal is inspired not only by research in the domains of sociology\comment{~\cite{hall1974,Kendon1990,Hoyle,Tannen}, economics~\cite{econo_category}, and education~\cite{Scherr2009}} which have analyzed a small number of salient and meaningful proxemes (e.g., conversational ``f-formations''~\cite{Kendon1990} in Fig.~\ref{fig:socialbehavior}(e)), but also by our preliminary results suggesting that these proxemes can reliably be detected and recognized in a real-world (classroom) setting. It is also motivated by our experiments showing that new domain-specific proxemes can be discovered in an unsupervised manner when data is abundant but domain-expert guidance is lacking.

%We begin with learning and recognizing interactions and then discuss inferring social network information in Section~\ref{sec:vis2net}. We describe datasets and challenge problems in Section~\ref{sec:sys}. 

%We propose to learn and detect interaction categories (Section \ref{sec:activity}), in complementary to those companion efforts for still images. We then propose to infer social network from images and videos by leveraging all socially-informative visual patterns including occurrences of interactions as an intermediate representation (Section \ref{sec:vis2net}).

%(b-1) Independently recognizing the two faces: The right face is hard to distinguish between two possible people; (b-2) Recognizing faces using social relationship inferred from (a-1) through (a-6): The right face is now more likely to be associated with one person than the other.

%Consider that in recognizing the two faces detected in Fig. \ref{fig:prob1} (a-1) the right face is hard to distinguish between `Susan' and `Helen' (see (b-1)). However, the inferred `friends' relationship suggests that the individual should belong to the left person's list of friends (see (b-2)), which includes only `Helen' but not `Susan'.

\subsection{Social proxemes: Learning, detection, and recognition}
\label{sec:activity}

%At the most basic level, learning and detecting interaction categories requires inventing two things: 1) a computational representation of interactions; and 2) a recipe for measuring the similarity (or distance) between any two interactions. In developing these two, we have four desiderata. First, they must be discriminative, meaning that the similarity between instances from distinct categories is low while that for instances from the same category is high. Second, they must be insensitive to distractors, such as changes in viewpoint, participant identities, the temporal extent of the interaction, and the rate of progression within the interaction. Third, they must be robust and inexpensive so that we can automatically and efficiently process large image collections and long videos of large social gatherings. Fourth and finally, they must be flexible in the sense of being able to automatically exploit the varying levels of interaction information available in different types of input, from low-resolution surveillance video to high-resolution videos captured in small meetings.


At the most basic level, learning and detecting proxemes requires inventing two things: 1) a computational representation of interactions (\emph{i.e.},instances of proxemes); and 2) a recipe for measuring the similarity (or distance) between two interactions. In developing these two, we have four desiderata. First, they must be discriminative, meaning that the similarity between instances from distinct categories is low while that for instances from the same category is high. Second, they must be insensitive to distractors, such as changes in viewpoint, participant identities, the temporal extent of the interaction, and the rate of progression within the interaction. Third, they must be robust and inexpensive so that we can automatically and efficiently process large image collections and long videos of large social gatherings. Fourth and finally, they must be flexible in \comment{the sense of being able to automatically} exploiting the varying levels of information available in different types of input, from low-resolution surveillance video to high-resolution images captured in small meetings.

These desiderata motivate us to abandon existing approaches such as coupled hidden Markov models (CHMMs)~\cite{Brand:CHMM} used for videos. These approaches are inspired by speech analysis and attempt to model an interaction as an explicit \emph{multi-thread event}~\cite{Hongeng:act} comprised of two or more single-thread events (i.e., two or more temporal sequences of per-person atomic actions) that are interconnected in time. These approaches can theoretically provide very robust detection and recognition of interactions, but before they can be applied one must optimize for each interaction category both the connectivity of the CHMM and its parameters. This is a daunting task that would seem to require hundreds of labeled training samples for each category.

%We propose to develop a new class of representations for social interactions that are better-suited to data-driven analysis, including use of techniques like domain adaptation, active learning, semi-supervised learning, and unsupervised learning, which can help reduce the number of training samples to enable widespread use. Our representations are based on ensembles of two types of time-varying descriptors: 1) per-agent descriptors that encode the appearance and/or motion of each agent; and 2) relative pairwise descriptors that encode the appearance and/or motion of each agent relative to another. In the simplest case, an instance of an interaction spanning $T$ time units (frames) and containing $M$ (possibly noisy and fragmented) space-time tracks of agents---these could be centroid locations, bounding boxes, silhouettes, etc.---is simply represented by the ensemble $\{\mathbf{f}_{m,t},\mathbf{g}_{m,m',t}\}$ of $M\times T$ per-agent descriptors $\mathbf{f}_{m,t}$ and $M\times (M-1)\times T$ per-pair descriptors $\mathbf{g}_{m,m',t}$. Computing the similarity between any two interactions amounts to comparing two ensembles, and detecting an interaction category in a long video of a larger social gather amounts to searching through space and time for ensembles that are similar to one or more exemplars in that category. 

We propose to develop a new class of representations for interactions that are better-suited to data-driven analysis. For videos, our representations are based on ensembles of two types of time-varying descriptors: 1) per-agent descriptors that encode the appearance and/or motion of each agent; and 2) relative pairwise descriptors that encode the appearance and/or motion of each agent relative to another. In the simplest case, an interaction spanning $T$ time units (frames) and containing $M$ (possibly noisy and fragmented) space-time tracks of agents---these could be centroid locations, bounding boxes, silhouettes, etc.---is simply represented by the ensemble $\{\mathbf{f}_{m,t},\mathbf{g}_{m,m',t}\}$ of $M\times T$ per-agent descriptors $\mathbf{f}_{m,t}$ and $M\times (M-1)\times T$ per-pair descriptors $\mathbf{g}_{m,m',t}$. For images, we use the same representation with $T=1$. Computing the similarity between any two interactions amounts to comparing two ensembles, and detecting a proxeme instance in a larger social gathering amounts to searching through space and/or time for ensembles that are similar to one or more exemplars of that proxeme. 

%Our ensemble-based approach has the advantage of avoiding the explicit semantic description of social interactions, which is important when an interaction category cannot easily be broken down according to any pre-defined grammar, or when one lacks the vocabulary to precisely describe the coupled sequences of atomic events that comprise it. It is flexible because the dimension and form of the descriptors $\mathbf{f}$ and $\mathbf{g}$ can be whatever is appropriate for the setting in question, from centroid position and velocity computed from low-resolution surveillance video to more sophisticated descriptors based on space-time interest points, histograms of flow, body pose, expressions, gestures, or higher-level individual action categories. Finally, as we will describe in the remainder of this section, it can be readily combined with matching techniques for comparisons that are insensitive to distractors, allowing applications from learning to detection and recognition.

Our ensemble-based approach has the advantage of avoiding the explicit semantic description of interactions, which is important when a proxeme cannot easily be broken down according to any pre-defined grammar, or when one lacks the vocabulary to precisely describe the coupled sequences of atomic events \comment{or compositions of atomic co-occurrences that comprise it}. It is flexible because the dimension and form of the descriptors $\mathbf{f}$ and $\mathbf{g}$ can be whatever is appropriate for the setting in question, from centroid position and velocity computed from low-resolution surveillance video to more sophisticated descriptors based on space-time interest points, histograms of flow, body pose, expressions, gestures, or higher-level individual action categories. Finally, as we will describe in the remainder of this section, it can be readily combined with matching techniques for comparisons that are insensitive to distractors, allowing applications from learning to detection and recognition.

%More precisely,  specifically, assume an input video consisting of $T$ frames. By applying domain-appropriate detection and tracking, assume we obtain $M$ space-time tracks of bounding boxes enclosing $M$ faces or bodies. The $M$ targets are to be compared with an interaction exemplar consisting of $S$ frames and $N$ targets ($N\le M$) that are correctly detected and tracked and all participating in an coherent interaction. Our representation for the input tracks therefore includes $M\times T$ $d_{I}$-dimensional descriptors $\{\mathbf{f}_{m,t}\}_{m=1,2,\cdots,M, t=1,2,\cdots,T}$, where $\mathbf{f}_{m,t}$ encodes the individual activity of the $m$th target at time $t$, and $M\times (M-1)\times T$ $d_{P}$-dimensional pairwise descriptors $\{\mathbf{g}_{m,m',t}\}_{m,m'=1,2,\cdots,M, m\neq m', t=1,2,\cdots,T}$, where $\mathbf{g}_{m,m',t}$ encodes dynamic visual properties of target $m$ exhibited relative to those of target $m'$ at time $t$. Meanwhile, the the interactions of interest are stored as exemplars in a gallery, each exemplar associated with a category label. For each exemplar, we have two similar descriptor collections $\{\mathbf{f}^{D}_{n,s}\}_{n=1,2,\cdots,N, s=1, 2,\cdots, S}$ and $\{\mathbf{g}^{D}_{n,n',s}\}_{n,n'=1,2,\cdots,N, n\neq n', s=1, 2,\cdots, S}$. We denote the descriptor ensemble for the input at time $t$ to be $\mathcal{Q}_{t}\triangleq\{\mathbf{f}_{m,t},\mathbf{g}_{m,m',t}\}$, and that for the exemplar at time $s$ as $\mathcal{D}_{s}\triangleq\{\mathbf{f}^{D}_{n,s},\mathbf{g}^{D}_{n,n',s}\} $. The input is then represented as $\mathcal{Q}\triangleq\{\mathcal{Q}_{t}\}_{1\leq t\leq T}$ and the exemplar $\mathcal{D}\triangleq\{\mathcal{D}_{s}\}_{1\leq s\leq S}$. We show that the can allow efficient measurement of distance in Section \ref{subsec:activity}, and then describe how it serves as a foundation for learning interaction categories in Section \ref{sec:actlearn}.


%We seek to develop new approaches for detecting, localizing, and recognizing these socially informative visual patterns, i.e., interactions, from videos, in complementary to those companion efforts for still images. Through this effort, we foresee a richer set of heterogenous socially informative visual cues from videos in addition to those from images, so as to enable high-level reasoning as to be introduced in Section \ref{sec:vis2net}. 
%We propose a paradigm as to be detailed in Section \ref{subsec:activity}, which is different from the existing work in three aspects. 

%Second, we propose to predict activities under social contexts. In this case, social semantics assist in visual understanding in the way that they serve as contextual information for restricting the more possible categories of activities that may occur between a specific group of individuals. This effort is also complementary to the usage of social contexts for identifying the targets presented in the previous section. Consider a simple illustrative example in which we would like to label a conversational scene in a movie as either a `negotiation‘ or a 'debate'. It is possible, in this case, that by the analysis of facial expressions, gestures, and poses, we still have difficulty in distinguishing the two. However, by appropriate face recognition in association with other metadata of the movie, probably with the help of social contexts as introduced in the previous section, we may gain solid confidence regarding the social relationship between the speakers as either `cooperative' or 'adverse'. A cooperative relationship are more likely to imply a negotiating activity, and an adverse relationship implies otherwise. A mechanism, similar to and in companion to the CRF formulation but adapted to video analysis, will be particularly useful.

%
%In our following research, we aim to enrich the descriptions of the social behaviors in the current prototype system, explore more accurate and robust comparison and retrieval mechanisms, and in particular, design novel modules for integrating social contextual annotations to allow our framework to be fully `socially-aware'.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\subsection{Joint Recognition of Multiple Targets with Social Contexts}
%\label{sec:recognition}
%
%The automatic recognition of targets in image and videos is an central task of computer vision. Recognition of objets is not only essential to any system that seeks to manage visual media based on content, but also necessary for automatically annotating images/videos (`auto-tagging') and indexing them. Recognition of faces is even integral to successfully interpreting images/videos of humans and consequently mining the social semantics in these imagery.
%
%Automatic recognition, since its initial stage until most recently, treats the targets as well as the outputted categories to be independent rom one to the other, which is essentially not the case. In order for automatic recognition to truly succeed, we must exploit the syntactic relationships between targets and between categories and use other contextual information. We in this section propose a scalable computational framework for socially-aware recognition systems that exploit contextual information from online social networks. These online communities provide two significant types of information which, to date, have only been scarcely exploited: 1) significant textual metadata, and 2) social network structure as context. The proposed activity seeks computational foundations for exploiting both.
%
%A typical example of socially-aware face recognition is as follows. A user uploads a small, hundred-image `album' (say)  and we seek to automatically recognize the identities of detected faces in the images. To achieve this goal, we represent the detected faces as nodes in a densely connected, undirected graph. With this representation, recognition consists of estimating a joint labeling of the nodes, which can be formulated as the optimization of an energy-based model. What is unique to our approach is that we seek energy models that combine image information (e.g., classifier output) with contextual information drawn from the user's online social network. Another example is scene recognition, where we seek to jointly label all images with their scene categories in several related online album containing thousands of scene images. Similar examples are more than the two mentioned examples.
%
%The description of an individual target is generally in multiple `views', where a view refers to a specific type of attribute that describes the target. A detected face, for example, can be described by skin color, texture, shape configurations of facial organs, etc.. The description of the contextual network relationship, meanwhile, is also in multiple views. The context between two faces(humans) in Facebook, for example, can be number of their shared friends, the number of their co-occurrence, and their relative poses in images, and so on. As we have mentioned, the social networks are usually partially observed, meaning that not all these views are `visible', and a practically useful recognition system must accommodate the un-observed views for the nodes or the links. Formally,  we can represent  the densely connected, undirected graph as $G$ describing the community of $K$ individuals, where $G\triangleq\{N^{(v_1)},A^{(v_2)},P^{(v_1)}, Q^{(v_2)}\}, v_1=1,2,\cdots,V_1,\hspace{5pt} v_2=1,2,\cdots,V_2$,  $N^{(v_1)}$ is a $K\times 1$ vector describing the individuals in the $v_1$th view, $P^{(v_1)}$ is a $K\times 1$ binary vector describing the visibility for that view, $A^{(v_2)}$ is the $K\times K$ affinity matrix describing the relational context for the $v_2$th view, and $Q^{(v_2)}$ is the $K\times K$ visibility matrix for that view. If $P^{(v_1)}(i)=1$, then $N^{(v_1)}(i)$ is the scalar description of the individual $i$ in the $v_1$th view; otherwise if $P^{(v_1)}(i)=0$ then $N^{(v_1)}(i)$ is a missing number indicating the lack of information of this node in this view. Similar notions apply for the matrices $A$ and $Q$.
%
%With this formulation, a natural computational framework for this socially-aware recognition is a conditional random field (CRF) model~\cite{lafferty2001crf, sutton2007icr, he2004mcr} with a node for each item of interest and an edge connecting many pairs of nodes. In this case, the goal is to infer a joint labeling $\vy=\{y_i\}$ of the relevant categories over all nodes $i$ in the graph. We use the notation $y_i\in{\cal L}=\{l_0\ldots l_M\}$ for the discrete label space of the nodes. (In general, these will vary from node to node, but we use a single set of labels here for notational convenience.) An optimal joint labeling is found by maximizing the conditional density
%\begin{equation} \label{eq:crf}
%{\rm Pr}(\vy|G) = \frac{1}{Z(G)}e^{-E(\vy|G)}
%\end{equation}
%where the partition function $Z(G)$ is a data-dependent normalizing constant and the energy $E(\vy|G)$ consists of sums of unary and pairwise potential functions at the nodes and edges:
%\begin{equation}\label{eq:energy}
%E(\vy|G) =  \sum_{i}  \phi_i (y_i|G) + \sum_{(i,j)} \phi_{ij}(y_i, y_j|G).
%\end{equation}
%
%In the CRF framework, the unary potential functions $\phi_i(y_i|G)$ capture information that is local to each node, and the pairwise potentials $\phi_{ij}(y_i, y_j|G)$ represent the compatibilities of possible label pairs across an edge. Therefore, assuming that the unary potentials depend on individual descriptions and pairwise potentials depend on contextual descriptions, (\ref{eq:energy}) can be decomposed as
%\begin{equation}\label{eq:energy_simple}
%E(\vy|G) =  \sum_{i}  \phi_i (y_i|N,P) + \sum_{(i,j)} \phi_{ij}(y_i, y_j|A,Q).
%\end{equation}
%As part of the proposed activity, we will study the application of this model to practical recognition tasks. Our goal is to answer the following three questions: 1) what types of social network context information improve recognition and by how much?; 2) how should multiple-view of information be weighted and how should missing observation be accounted for?; and 3) how can we apply this model at a practical scale? 
%
%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=3.5in]{ivw_facescores}
%\end{center}
%\vspace{-0.25in} \caption{\captionsize 
%The performance of a commercial face recognition system on tagged faces harvested from Facebook~\cite{Stone2008}. Given the query face in the upper left, the system has returned the remaining faces as being most similar. Similarity to the query decreases from left to right and from top to bottom, and the ground-truth matches are highlighted with green squares. Due to the variability of this `real-world' dataset, the correct matches are not highly ranked.\label{fig:face-results}\afterfigspace}
%\end{figure}
%
%
%
%\SubSubSection{Preliminary Study}\label{sec:prelim-face-results}
%Evidence for the utility of social network context is provided under simplified assumptions by the work of PI Zickler~\cite{Stone2008,Stone2010}, the former of which received the Best Paper Award at the First IEEE Workshop on Internet Vision. This work reported the results of a preliminary study on the ability of social network context to improve face-based identity recognition using a subset of 2641 labeled faces from Facebook. In this study, we restricted our attention to images with two faces (two-node graphs) so that CRF training and inference could be achieved without approximation. The label space ${\cal L}=\{l_0,\ldots,l_M\}$  consists of a set of possible identities, and we considered a simple set of individual node descriptions and contextual relationship descriptions. Specifically, we assumed that all descriptions in all views are observed and error-free, in which case the potentials in (\ref{eq:energy_simple}) are further expanded as a linear combination of view-specific univariate and bivariate \emph{feature functions} independent of observability variables $P,Q$:
%\begin{eqnarray}
%\phi_i(y_i|N,P) &=& \sum_{v_1} \alpha_{v_1}(N^{(v_1)}) f_{v_1}(y_i, N^{(v_1)})\\
%\phi_{ij}(y_i, y_j|A,Q) &=& \sum_{v_2} \beta_{v_2}(A^{(v_2)}) g_{v_2}(y_i, y_j, A^{(v_2)}).
%\end{eqnarray}
%
%
%\begin{figure}[t!]
%\begin{center}
%\includegraphics[width=5in]{rank_graph_small}\\
%{\footnotesize
%\begin{tabular}{ll}
%\hline\hline\multicolumn{2}{l}{\raisebox{-0.1in}{Univariate Functions $f_{v_1}(y_i, N^{(v_1)})$}} \\
%\hline
%\parbox[t]{1.8in}{\raggedright Face score (\textsf{face})} & \parbox[t]{3.5in}{A distribution of likelihoods over ${\cal L}$ returned by a commercial face recognition system (see Fig.~\ref{fig:face-results})} \vspace{0.03in}\\
%\parbox[t]{1.8in}{\raggedright Photo history (\textsf{hist})} & \parbox[t]{3.5in}{Number of times each individual $l_m$ has been tagged in previous photos posted by the photographer} \vspace{0.07in}\\
%\multicolumn{2}{l}{Bivariate Functions $ g_{v_2}(y_i, y_j, A^{(v_2)})$} \\
%\hline
%\parbox[t]{1.8in}{\raggedright Friendship (\textsf{friend})}& \parbox[t]{3.5in}{Binary indicator that is one iff individuals $l_m$ and $l_n$ are `Facebook friends' } \vspace{0.04in}\\
%\parbox[t]{1.8in}{\raggedright Pairwise co-occurrence (\textsf{pair})}& \parbox[t]{3.5in}{Number of times each pair of individuals $(l_m,l_n)$ has been jointly tagged in previous photos posted by the photographer}\\
%\end{tabular}}
%\end{center}
%\captionspace \caption{\captionsize 
%Face recognition performance as a function of rank threshold for a variety of combinations of feature functions (described bottom). At each rank value $R$, the graph displays the proportion of all test samples for which the correct ground-truth identity label appeared in the top $R$ predictions. Social network context improves recognition, and different sources of context information are complimentary\cite{Stone2008}.\label{fig:ivw-curves}\afterfigspace}
%\end{figure}
%
%
%The results are summarized in Figs.~\ref{fig:face-results} and \ref{fig:ivw-curves}. For each combination of feature functions, we determined model parameters ($\alpha$ and $\beta$) by maximizing the conditional log-likelihood of a training set by gradient ascent. Then, for each left-out test photo, exact marginal probabilities were computed for the test photo's face-nodes, and  the outputs were compared against ground truth. Specifically, the marginal probabilities were used to compute a ranked list at each node, and we measured how often the correct identity label appeared in the top $R$ ranks for a sliding value of $R$ (c.f.~\cite{frvt06}).
%
%A number of observations can be drawn from these results. First, even when using the face score (\textsf{face}) alone (see Fig.~\ref{fig:face-results}) social network context plays a vital role. In our data, over 99\% of tagged faces correspond to `friends' of the photographer, and since the identity of the photographer is known (it is included in the input $N$), we can safely reduce the label space ${\cal L}$ from the 15,752 individuals in the global gallery to the hundreds of friends of the input photographer. Without this restriction, the results in Fig.~\ref{fig:face-results} would degrade remarkably. The results in Fig.~\ref{fig:ivw-curves} also demonstrate that social network context substantially improves baseline face recognition, and that different sources of context information are complementary.
%
%
%These preliminary results are encouraging, especially since the network context information that was employed barely scratches the surface of possibilities. Existing work shows that clothing can provide useful information within an album~\cite{anguelov2007cir, zhang2003aah,  song2006cah, sivic2006fpr}, and it is possible that clothing could be used more globally as well, since the long-term clothing trends of certain individuals may be distinguishable from those of others. Recognition of facial attributes~\cite{LNCS53050340}, such as glasses or facial hair, can be used in conjunction with identity scores to improve recognition, and the same is true for gender recognition, which in many online communities is a knowable piece of information. In addition, people associate with each other in homes, schools, outdoors, workplaces, clubs, and so on, and a more explicit representation of these would be beneficial. Certain individuals will appear more often in certain types of scenes, and these `likely' scene types will change depending on who they are with. Temporal and geographic information, when available, may be further predictive of which individual is likely to occur and with whom. 
%
%
%
%In our proposed research, we aim to both qualitatively and quantitatively analyze large collections of images and their embedding network to discover the utility of these multi-view context information, and in particular, explore effective approaches to account for incompletely observed graph, which is the crucial to allow our framework to succeed in realistic network conditions.